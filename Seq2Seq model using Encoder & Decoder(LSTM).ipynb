{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf39765f",
   "metadata": {},
   "source": [
    "### Objective :  \n",
    "- We will learn to implement a sequence to sequence model which can tranlate German(encoder) to English(decoder) Language.\n",
    "\n",
    "### Dataset Used\n",
    "- The dataset we'll be using is the Multi30k dataset. This is a dataset with ~30,000 parallel English, German and French sentences, each with ~12 words per sentence.\n",
    "\n",
    "### Introduction to  sequence-to-sequence(seq2seq):\n",
    "- **The most common sequence-to-sequence (seq2seq) models are encoder-decoder models, which commonly use a recurrent neural network (RNN) to encode the source (input) sentence into a single vector.**\n",
    "- **Then this single vector also called as context vector is then decoded by a second RNN which learns to output the target (output) sentence by generating it one word at a time.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c7cea0",
   "metadata": {},
   "source": [
    "#### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9da57e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\sequence\\sequence\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa9afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use seed to get deterministic results\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45740e7",
   "metadata": {},
   "source": [
    "#### Download models  ` de_core_news_sm` & `en_core_web_sm` from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37468069",
   "metadata": {},
   "source": [
    "#### Create the tokenizer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee8ea76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_german(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6125d6c",
   "metadata": {},
   "source": [
    "#### Field in Torchtext = handle how our data should be processed\n",
    "- tokenize argument with German being the SRC (source) field and English being the TRG (target) field.\n",
    "- The field also appends the \"start of sequence\" and \"end of sequence\" tokens via the init_token and eos_token arguments, and converts all words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4878271",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = Field(tokenize= tokenize_german,\n",
    "              init_token='<sos>',\n",
    "              eos_token='<eos>',\n",
    "              lower=True)\n",
    "TARGET = Field(tokenize=tokenize_english,\n",
    "              init_token='<sos>',\n",
    "              eos_token='<eos>',\n",
    "              lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a34dfd",
   "metadata": {},
   "source": [
    "#### Download Multi30k dataset\n",
    "- exts specifies which languages to use as the source and target (source goes first) and fields specifies which field to use for the source and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca79b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data,test_data = Multi30k.splits(exts = ('.de','.en'),fields=(SOURCE,TARGET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aea5237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0196f1a",
   "metadata": {},
   "source": [
    "#### Build  vocabulary for the source and target languages.\n",
    "- `min_freq` argument, we only allow tokens that appear at least 2 times to appear in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f67192",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE.build_vocab(train_data, min_freq = 2)\n",
    "TARGET.build_vocab(train_data,min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2108cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of source vocabulary 7853\n",
      "length of target vocabulary 5893\n"
     ]
    }
   ],
   "source": [
    "print(\"length of source vocabulary\", len(SOURCE.vocab))\n",
    "print(\"length of target vocabulary\", len(TARGET.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb6106ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651bb00e",
   "metadata": {},
   "source": [
    "#### Create Iterators\n",
    "- `BucketIterator`  creates batches in such a way that it minimizes the amount of padding in both the source and target sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf270e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9948fe",
   "metadata": {},
   "source": [
    "#### Building the Seq2Seq Model\n",
    "**1. Encoder**\n",
    "- The encoder, a 2 layer LSTM and Encoder takes the following arguments\n",
    "- `input_dim` is the size of the one-hot vectors that will be input to the encoder. This is equal to the input (source) vocabulary size.\n",
    "- `emb_dim` is the dimensionality of the embedding layer. This layer converts the one-hot vectors into dense vectors with `emb_dim` dimensions. \n",
    "- `hid_dim` is the dimensionality of the hidden and cell states.\n",
    "- `n_layers` is the number of layers in the RNN.\n",
    "- `dropout` is the amount of dropout to use. This is a regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f2a29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim,emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim,hid_dim,n_layers,dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        #src = [src len, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        outputs,(hidden,cell) = self.rnn(embedded)\n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4958432",
   "metadata": {},
   "source": [
    "**2. Decoder**\n",
    "-  Decoder will also be  a 2 layer LSTM.\n",
    "- The arguments and initialization are similar to the Encoder class,\n",
    "- except we now have an output_dim which is the size of the vocabulary for the output/target. \n",
    "- There is also the addition of the Linear layer, used to make the predictions from the top layer hidden state.\n",
    "- Within the forward method, we accept a batch of input tokens, previous hidden states and previous cell states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ccaa311",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim,emb_dim,hid_dim,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim,emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim,n_layers,dropout=dropout)\n",
    "        self.fc = nn.Linear(hid_dim,output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,inp, hidden,cell):\n",
    "        inp = inp.unsqueeze(0) # We unsqueeze the input tokens to add a sentence length dimension of 1\n",
    "        embedded = self.dropout(self.embedding(inp))\n",
    "        output,(hidden,cell) = self.rnn(embedded,(hidden,cell))\n",
    "        prediction = self.fc(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4479cc7",
   "metadata": {},
   "source": [
    "**3.Seq2Seq**\n",
    "- For the final part of the implemenetation, we'll implement the seq2seq model.\n",
    "    - RECEIVING THE INPUT SENTENCE\n",
    "    - USE ENCODER TO PRODUCE CONTEXT VECTOR\n",
    "    - USE DECODE TO PROUCE THE PRDICTED SENTENCE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c645f",
   "metadata": {},
   "source": [
    "- The Seq2Seq model takes in an Encoder, Decoder, and a device.\n",
    "- we have to ensure that the number of layers and the hidden (and cell) dimensions are equal in the Encoder and Decoder.\n",
    "- Our forward method takes the source sentence, target sentence and a teacher-forcing ratio.\n",
    "- The first thing we do in the forward method is to create an outputs tensor that will store all of our predictions.\n",
    "- We then feed the input sentence, into the encoder and receive out final hidden and cell states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc739494",
   "metadata": {},
   "source": [
    "During each iteration of the loop, we:\n",
    "- pass the input, previous hidden and previous cell states ($y_t, s_{t-1}, c_{t-1}$) into the decoder\n",
    "- receive a prediction, next hidden state and next cell state ($\\hat{y}_{t+1}, s_{t}, c_{t}$) from the decoder\n",
    "- place our prediction, $\\hat{y}_{t+1}$/`output` in our tensor of predictions, $\\hat{Y}$/`outputs`\n",
    "- decide if we are going to \"teacher force\" or not\n",
    "    - if we do, the next `input` is the ground-truth next token in the sequence, $y_{t+1}$/`trg[t]`\n",
    "    - if we don't, the next `input` is the predicted next token in the sequence, $\\hat{y}_{t+1}$/`top1`, which we get by doing an `argmax` over the output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a19f4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence2Sequence(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \"Hidden dimensions must be Equal\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \"encoder and decoder should have equal no of layers\"\n",
    "        \n",
    "    def forward(self, source, target, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = target.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(target_len,batch_size, target_vocab_size).to(device)\n",
    "        hidden, cell = self.encoder(source)\n",
    "        inp= target[0,:]\n",
    "        \n",
    "        for t in range(1,target_len):\n",
    "            output, hidden, cell = self.decoder(inp, hidden,cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top = output.argmax(1)\n",
    "            inp = target[t] if teacher_force else top\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2d3dc",
   "metadata": {},
   "source": [
    "#### Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f101fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SOURCE.vocab)\n",
    "OUTPUT_DIM = len(TARGET.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Sequence2Sequence(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e631b02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence2Sequence(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa55564c",
   "metadata": {},
   "source": [
    "Next up is initializing the weights of our model. In the actual paper they initialize all weights from a uniform distribution between -0.08 and +0.08, i.e. $\\mathcal{U}(-0.08, 0.08)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d233239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        nn.init.uniform_(param.data,-0.08,0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e1d20fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence2Sequence(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c628c8ba",
   "metadata": {},
   "source": [
    "#### Define our optimizer, which we use to update our parameters in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1113b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ce248",
   "metadata": {},
   "source": [
    "#### Define our loss function-  CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f232ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PAD_INDEX = TARGET.vocab.stoi[TARGET.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bda93ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=TARGET_PAD_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24b6127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # the loss function only works on 2d inputs with 1d targets we need to flatten each of them with .view\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be76237",
   "metadata": {},
   "source": [
    "#### Evaluating the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52e45d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "233273c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8a001ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 0s\n",
      "\tTrain Loss: 5.064 | Train PPL: 158.249\n",
      "\t Val. Loss: 4.940 |  Val. PPL: 139.814\n",
      "Epoch: 02 | Time: 1m 18s\n",
      "\tTrain Loss: 4.507 | Train PPL:  90.687\n",
      "\t Val. Loss: 4.823 |  Val. PPL: 124.390\n",
      "Epoch: 03 | Time: 0m 34s\n",
      "\tTrain Loss: 4.199 | Train PPL:  66.639\n",
      "\t Val. Loss: 4.635 |  Val. PPL: 102.979\n",
      "Epoch: 04 | Time: 0m 50s\n",
      "\tTrain Loss: 3.973 | Train PPL:  53.135\n",
      "\t Val. Loss: 4.451 |  Val. PPL:  85.702\n",
      "Epoch: 05 | Time: 0m 39s\n",
      "\tTrain Loss: 3.800 | Train PPL:  44.693\n",
      "\t Val. Loss: 4.351 |  Val. PPL:  77.573\n",
      "Epoch: 06 | Time: 0m 49s\n",
      "\tTrain Loss: 3.654 | Train PPL:  38.633\n",
      "\t Val. Loss: 4.221 |  Val. PPL:  68.099\n",
      "Epoch: 07 | Time: 1m 9s\n",
      "\tTrain Loss: 3.515 | Train PPL:  33.624\n",
      "\t Val. Loss: 4.155 |  Val. PPL:  63.740\n",
      "Epoch: 08 | Time: 1m 29s\n",
      "\tTrain Loss: 3.379 | Train PPL:  29.350\n",
      "\t Val. Loss: 4.065 |  Val. PPL:  58.289\n",
      "Epoch: 09 | Time: 1m 22s\n",
      "\tTrain Loss: 3.251 | Train PPL:  25.828\n",
      "\t Val. Loss: 4.015 |  Val. PPL:  55.438\n",
      "Epoch: 10 | Time: 1m 20s\n",
      "\tTrain Loss: 3.164 | Train PPL:  23.661\n",
      "\t Val. Loss: 3.921 |  Val. PPL:  50.466\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa5b83ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.906 | Test PPL:  49.694 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sequence",
   "language": "python",
   "name": "sequence"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
